#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®åˆ†æå™¨ - è‡ªåŠ¨è¯»å–å¹¶åˆ†ædataç›®å½•ä¸‹çš„æ‰€æœ‰æ•°æ®æ–‡ä»¶
æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼šCSV, Parquet, DuckDB
"""

import os
import pandas as pd
import duckdb
from pathlib import Path
from typing import Dict, Any, List
import warnings
warnings.filterwarnings('ignore')


class DataAnalyzer:
    """æ•°æ®åˆ†æå™¨ç±»ï¼Œç”¨äºè‡ªåŠ¨è¯»å–å’Œåˆ†æå„ç§æ ¼å¼çš„æ•°æ®æ–‡ä»¶"""
    
    def __init__(self, data_dir: str = None):
        """åˆå§‹åŒ–æ•°æ®åˆ†æå™¨
        
        Args:
            data_dir: æ•°æ®ç›®å½•è·¯å¾„ï¼Œé»˜è®¤ä¸ºé¡¹ç›®æ ¹ç›®å½•ä¸‹çš„dataæ–‡ä»¶å¤¹
        """
        if data_dir is None:
            # è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•çš„ä¸Šçº§ç›®å½•ï¼Œç„¶åæ‹¼æ¥dataè·¯å¾„
            current_dir = Path(__file__).parent.parent
            self.data_dir = current_dir / "data"
        else:
            self.data_dir = Path(data_dir)
        
        if not self.data_dir.exists():
            raise FileNotFoundError(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {self.data_dir}")
    
    def get_data_files(self) -> List[Path]:
        """è·å–æ•°æ®ç›®å½•ä¸‹æ‰€æœ‰æ”¯æŒçš„æ•°æ®æ–‡ä»¶
        
        Returns:
            æ”¯æŒçš„æ•°æ®æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        supported_extensions = {'.csv', '.parquet', '.duckdb', '.db'}
        data_files = []
        
        for file_path in self.data_dir.iterdir():
            if file_path.is_file() and file_path.suffix.lower() in supported_extensions:
                data_files.append(file_path)
        
        return sorted(data_files)
    
    def read_csv_file(self, file_path: Path) -> pd.DataFrame:
        """è¯»å–CSVæ–‡ä»¶"""
        try:
            # å°è¯•ä¸åŒçš„ç¼–ç æ ¼å¼å’Œåˆ†éš”ç¬¦
            encodings = ['utf-8', 'utf-8-sig', 'gbk', 'gb2312', 'gb18030', 'utf-16']
            separators = [',', '\t', ';', '|']
            
            for encoding in encodings:
                for sep in separators:
                    try:
                        df = pd.read_csv(file_path, encoding=encoding, sep=sep)
                        # å¦‚æœåªæœ‰ä¸€åˆ—ä½†åŒ…å«åˆ¶è¡¨ç¬¦ï¼Œç›´æ¥å°è¯•åˆ¶è¡¨ç¬¦åˆ†éš”
                        if df.shape[1] == 1 and '\t' in df.columns[0]:
                            try:
                                df_tab = pd.read_csv(file_path, encoding=encoding, sep='\t')
                                if df_tab.shape[1] > 1:
                                    print(f"âœ“ æˆåŠŸè¯»å–CSVæ–‡ä»¶ (ç¼–ç : {encoding}, åˆ†éš”ç¬¦: '\t'): {file_path.name}")
                                    return df_tab
                            except Exception:
                                pass
                        # æ£€æŸ¥æ˜¯å¦æˆåŠŸè§£æï¼ˆåˆ—æ•°å¤§äº1æˆ–è€…æœ‰åˆç†çš„æ•°æ®ï¼‰
                        if df.shape[1] > 1 or (df.shape[1] == 1 and not df.columns[0].startswith('Ã¿Ã¾') and '\t' not in df.columns[0]):
                            print(f"âœ“ æˆåŠŸè¯»å–CSVæ–‡ä»¶ (ç¼–ç : {encoding}, åˆ†éš”ç¬¦: '{sep}'): {file_path.name}")
                            return df
                    except (UnicodeDecodeError, pd.errors.EmptyDataError, pd.errors.ParserError):
                        continue
            
            # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨latin-1ç¼–ç 
            try:
                df = pd.read_csv(file_path, encoding='latin-1', sep='\t')
                print(f"âš  ä½¿ç”¨latin-1ç¼–ç è¯»å–CSVæ–‡ä»¶: {file_path.name}")
                return df
            except Exception:
                print(f"âœ— æ‰€æœ‰ç¼–ç æ ¼å¼éƒ½æ— æ³•è¯»å–CSVæ–‡ä»¶: {file_path.name}")
                return None
            
        except Exception as e:
            print(f"âœ— è¯»å–CSVæ–‡ä»¶å¤±è´¥: {file_path.name}, é”™è¯¯: {e}")
            return None
    
    def read_parquet_file(self, file_path: Path) -> pd.DataFrame:
        """è¯»å–Parquetæ–‡ä»¶"""
        try:
            df = pd.read_parquet(file_path)
            print(f"âœ“ æˆåŠŸè¯»å–Parquetæ–‡ä»¶: {file_path.name}")
            return df
        except Exception as e:
            print(f"âœ— è¯»å–Parquetæ–‡ä»¶å¤±è´¥: {file_path.name}, é”™è¯¯: {e}")
            return None
    
    def read_duckdb_file(self, file_path: Path) -> Dict[str, pd.DataFrame]:
        """è¯»å–DuckDBæ–‡ä»¶ä¸­çš„æ‰€æœ‰è¡¨"""
        try:
            conn = duckdb.connect(str(file_path))
            
            # è·å–æ‰€æœ‰è¡¨å
            tables_query = "SELECT table_name FROM information_schema.tables WHERE table_schema = 'main'"
            tables_result = conn.execute(tables_query).fetchall()
            table_names = [row[0] for row in tables_result]
            
            if not table_names:
                print(f"âš  DuckDBæ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°è¡¨: {file_path.name}")
                conn.close()
                return {}
            
            tables_data = {}
            for table_name in table_names:
                try:
                    df = conn.execute(f"SELECT * FROM {table_name}").df()
                    tables_data[table_name] = df
                    print(f"âœ“ æˆåŠŸè¯»å–DuckDBè¡¨: {file_path.name}.{table_name}")
                except Exception as e:
                    print(f"âœ— è¯»å–DuckDBè¡¨å¤±è´¥: {file_path.name}.{table_name}, é”™è¯¯: {e}")
            
            conn.close()
            return tables_data
            
        except Exception as e:
            print(f"âœ— è¿æ¥DuckDBæ–‡ä»¶å¤±è´¥: {file_path.name}, é”™è¯¯: {e}")
            return {}
    
    def describe_dataframe(self, df: pd.DataFrame, name: str) -> Dict[str, Any]:
        """å¯¹DataFrameè¿›è¡ŒåŸºæœ¬æè¿°ç»Ÿè®¡
        
        Args:
            df: è¦åˆ†æçš„DataFrame
            name: æ•°æ®é›†åç§°
            
        Returns:
            åŒ…å«æè¿°ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        if df is None or df.empty:
            return {"error": "æ•°æ®ä¸ºç©ºæˆ–æ— æ•ˆ"}
        
        description = {
            "æ•°æ®é›†åç§°": name,
            "æ•°æ®å½¢çŠ¶": df.shape,
            "è¡Œæ•°": df.shape[0],
            "åˆ—æ•°": df.shape[1],
            "åˆ—å": list(df.columns),
            "æ•°æ®ç±»å‹": df.dtypes.to_dict(),
            "ç¼ºå¤±å€¼ç»Ÿè®¡": df.isnull().sum().to_dict(),
            "å†…å­˜ä½¿ç”¨": f"{df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB"
        }
        
        # æ•°å€¼åˆ—çš„æè¿°ç»Ÿè®¡
        numeric_cols = df.select_dtypes(include=['number']).columns
        if len(numeric_cols) > 0:
            description["æ•°å€¼åˆ—æè¿°ç»Ÿè®¡"] = df[numeric_cols].describe().to_dict()
        
        # æ–‡æœ¬åˆ—çš„åŸºæœ¬ä¿¡æ¯
        text_cols = df.select_dtypes(include=['object', 'string']).columns
        if len(text_cols) > 0:
            text_info = {}
            for col in text_cols:
                text_info[col] = {
                    "å”¯ä¸€å€¼æ•°é‡": df[col].nunique(),
                    "æœ€å¸¸è§å€¼": df[col].mode().iloc[0] if not df[col].mode().empty else None
                }
            description["æ–‡æœ¬åˆ—ä¿¡æ¯"] = text_info
        
        return description
    
    def print_description(self, description: Dict[str, Any]):
        """æ ¼å¼åŒ–æ‰“å°æ•°æ®æè¿°ä¿¡æ¯"""
        print("\n" + "="*60)
        print(f"ğŸ“Š æ•°æ®é›†: {description.get('æ•°æ®é›†åç§°', 'Unknown')}")
        print("="*60)
        
        if "error" in description:
            print(f"âŒ é”™è¯¯: {description['error']}")
            return
        
        print(f"ğŸ“ æ•°æ®å½¢çŠ¶: {description['æ•°æ®å½¢çŠ¶']} (è¡Œæ•°: {description['è¡Œæ•°']}, åˆ—æ•°: {description['åˆ—æ•°']})")
        print(f"ğŸ’¾ å†…å­˜ä½¿ç”¨: {description['å†…å­˜ä½¿ç”¨']}")
        
        print("\nğŸ“‹ åˆ—ä¿¡æ¯:")
        for i, (col, dtype) in enumerate(description['æ•°æ®ç±»å‹'].items(), 1):
            missing = description['ç¼ºå¤±å€¼ç»Ÿè®¡'].get(col, 0)
            missing_pct = (missing / description['è¡Œæ•°'] * 100) if description['è¡Œæ•°'] > 0 else 0
            print(f"  {i:2d}. {col:<20} | ç±»å‹: {str(dtype):<10} | ç¼ºå¤±: {missing:>6} ({missing_pct:5.1f}%)")
        
        # æ•°å€¼åˆ—ç»Ÿè®¡
        if "æ•°å€¼åˆ—æè¿°ç»Ÿè®¡" in description:
            print("\nğŸ“ˆ æ•°å€¼åˆ—ç»Ÿè®¡:")
            numeric_stats = description["æ•°å€¼åˆ—æè¿°ç»Ÿè®¡"]
            for col in numeric_stats:
                stats = numeric_stats[col]
                print(f"  {col}:")
                print(f"    å‡å€¼: {stats.get('mean', 'N/A'):>10.2f} | æ ‡å‡†å·®: {stats.get('std', 'N/A'):>10.2f}")
                print(f"    æœ€å°å€¼: {stats.get('min', 'N/A'):>8.2f} | æœ€å¤§å€¼: {stats.get('max', 'N/A'):>10.2f}")
        
        # æ–‡æœ¬åˆ—ä¿¡æ¯
        if "æ–‡æœ¬åˆ—ä¿¡æ¯" in description:
            print("\nğŸ“ æ–‡æœ¬åˆ—ä¿¡æ¯:")
            text_info = description["æ–‡æœ¬åˆ—ä¿¡æ¯"]
            for col, info in text_info.items():
                print(f"  {col}: å”¯ä¸€å€¼ {info['å”¯ä¸€å€¼æ•°é‡']}, æœ€å¸¸è§: '{info['æœ€å¸¸è§å€¼']}'")
    
    def analyze_all_data(self):
        """åˆ†ææ‰€æœ‰æ•°æ®æ–‡ä»¶"""
        print(f"ğŸ” å¼€å§‹åˆ†ææ•°æ®ç›®å½•: {self.data_dir}")
        
        data_files = self.get_data_files()
        if not data_files:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æ”¯æŒçš„æ•°æ®æ–‡ä»¶")
            return
        
        print(f"ğŸ“ æ‰¾åˆ° {len(data_files)} ä¸ªæ•°æ®æ–‡ä»¶")
        
        total_datasets = 0
        
        for file_path in data_files:
            print(f"\nğŸ”„ å¤„ç†æ–‡ä»¶: {file_path.name}")
            
            if file_path.suffix.lower() == '.csv':
                df = self.read_csv_file(file_path)
                if df is not None:
                    description = self.describe_dataframe(df, file_path.name)
                    self.print_description(description)
                    total_datasets += 1
            
            elif file_path.suffix.lower() == '.parquet':
                df = self.read_parquet_file(file_path)
                if df is not None:
                    description = self.describe_dataframe(df, file_path.name)
                    self.print_description(description)
                    total_datasets += 1
            
            elif file_path.suffix.lower() in ['.duckdb', '.db']:
                tables_data = self.read_duckdb_file(file_path)
                for table_name, df in tables_data.items():
                    description = self.describe_dataframe(df, f"{file_path.name}.{table_name}")
                    self.print_description(description)
                    total_datasets += 1
        
        print(f"\nğŸ‰ åˆ†æå®Œæˆï¼å…±å¤„ç†äº† {total_datasets} ä¸ªæ•°æ®é›†")


def main():
    """ä¸»å‡½æ•°"""
    try:
        analyzer = DataAnalyzer()
        analyzer.analyze_all_data()
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")


if __name__ == "__main__":
    main()