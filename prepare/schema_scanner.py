#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®åº“Schemaè‡ªåŠ¨æ‰«æå™¨

è‡ªåŠ¨æ‰«ædata/ç›®å½•ä¸‹çš„æ‰€æœ‰æ•°æ®æ–‡ä»¶ï¼Œæå–è¡¨ç»“æ„ä¿¡æ¯
æ”¯æŒCSVã€Parquetã€DuckDBç­‰æ ¼å¼
è¿”å›ç»“æ„åŒ–çš„schemaå­—å…¸
"""

import os
import pandas as pd
import duckdb
from pathlib import Path
from typing import Dict, Any, List, Union
import warnings
warnings.filterwarnings('ignore')


class DatabaseSchemaScanner:
    """æ•°æ®åº“Schemaæ‰«æå™¨"""
    
    def __init__(self, data_dir: str = None):
        """åˆå§‹åŒ–Schemaæ‰«æå™¨
        
        Args:
            data_dir: æ•°æ®ç›®å½•è·¯å¾„ï¼Œé»˜è®¤ä¸ºé¡¹ç›®æ ¹ç›®å½•ä¸‹çš„dataæ–‡ä»¶å¤¹
        """
        if data_dir is None:
            # è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•çš„ä¸Šçº§ç›®å½•ï¼Œç„¶åæ‹¼æ¥dataè·¯å¾„
            current_dir = Path(__file__).parent.parent
            self.data_dir = current_dir / "data"
        else:
            self.data_dir = Path(data_dir)
        
        if not self.data_dir.exists():
            raise FileNotFoundError(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {self.data_dir}")
    
    def get_data_files(self) -> List[Path]:
        """è·å–æ•°æ®ç›®å½•ä¸‹æ‰€æœ‰æ”¯æŒçš„æ•°æ®æ–‡ä»¶
        
        Returns:
            æ”¯æŒçš„æ•°æ®æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        supported_extensions = {'.csv', '.parquet', '.duckdb', '.db'}
        data_files = []
        
        for file_path in self.data_dir.iterdir():
            if file_path.is_file() and file_path.suffix.lower() in supported_extensions:
                data_files.append(file_path)
        
        return sorted(data_files)
    
    def get_dataframe_schema(self, df: pd.DataFrame, table_name: str) -> Dict[str, Any]:
        """è·å–DataFrameçš„schemaä¿¡æ¯
        
        Args:
            df: pandas DataFrame
            table_name: è¡¨å
            
        Returns:
            è¡¨çš„schemaä¿¡æ¯å­—å…¸
        """
        if df is None or df.empty:
            return {
                "table_name": table_name,
                "row_count": 0,
                "column_count": 0,
                "columns": {},
                "error": "æ•°æ®ä¸ºç©ºæˆ–æ— æ•ˆ"
            }
        
        # æ„å»ºåˆ—ä¿¡æ¯
        columns_info = {}
        for col in df.columns:
            dtype = str(df[col].dtype)
            
            # è½¬æ¢pandasæ•°æ®ç±»å‹ä¸ºæ›´é€šç”¨çš„ç±»å‹
            if 'int' in dtype:
                data_type = 'INTEGER'
            elif 'float' in dtype:
                data_type = 'FLOAT'
            elif 'bool' in dtype:
                data_type = 'BOOLEAN'
            elif 'datetime' in dtype:
                data_type = 'DATETIME'
            elif 'object' in dtype or 'string' in dtype:
                data_type = 'TEXT'
            else:
                data_type = dtype.upper()
            
            # å¤„ç†sample_valuesï¼Œç¡®ä¿JSONå¯åºåˆ—åŒ–
            sample_values = []
            try:
                samples = df[col].dropna().head(3)
                for val in samples:
                    if pd.isna(val):
                        sample_values.append(None)
                    elif hasattr(val, 'isoformat'):  # datetimeç±»å‹
                        sample_values.append(val.isoformat())
                    elif hasattr(val, 'item'):  # numpyç±»å‹
                        sample_values.append(val.item())
                    else:
                        sample_values.append(str(val))
            except Exception:
                sample_values = []
            
            columns_info[col] = {
                "data_type": data_type,
                "pandas_dtype": dtype,
                "nullable": bool(df[col].isnull().any()),
                "null_count": int(df[col].isnull().sum()),
                "unique_count": int(df[col].nunique()),
                "sample_values": sample_values
            }
        
        return {
            "table_name": table_name,
            "row_count": int(df.shape[0]),
            "column_count": int(df.shape[1]),
            "columns": columns_info,
            "memory_usage_mb": round(df.memory_usage(deep=True).sum() / 1024 / 1024, 2)
        }
    
    def scan_csv_file(self, file_path: Path) -> Dict[str, Any]:
        """æ‰«æCSVæ–‡ä»¶çš„schema
        
        Args:
            file_path: CSVæ–‡ä»¶è·¯å¾„
            
        Returns:
            æ–‡ä»¶çš„schemaä¿¡æ¯
        """
        try:
            # å°è¯•ä¸åŒçš„ç¼–ç æ ¼å¼è¯»å–CSV
            encodings = ['utf-8', 'utf-8-sig', 'utf-16', 'utf-16-le', 'utf-16-be', 'gbk', 'gb2312', 'gb18030']
            separators = [',', '\t', ';', '|']
            
            df = None
            for encoding in encodings:
                for sep in separators:
                    try:
                        df_sample = pd.read_csv(file_path, encoding=encoding, sep=sep, nrows=1000)  # åªè¯»å–å‰1000è¡Œç”¨äºschemaåˆ†æ
                        if df_sample.shape[1] > 1 or (df_sample.shape[1] == 1 and not df_sample.columns[0].startswith('Ã¿Ã¾') and '\t' not in df_sample.columns[0]):
                            # è¯»å–å®Œæ•´æ–‡ä»¶è·å–å‡†ç¡®çš„è¡Œæ•°
                            df_full = pd.read_csv(file_path, encoding=encoding, sep=sep)
                            df = df_full
                            break
                    except Exception:
                        continue
                if df is not None:
                    break
            
            if df is None:
                return {
                    "file_name": file_path.name,
                    "file_type": "CSV",
                    "tables": {},
                    "error": "æ— æ³•è¯»å–CSVæ–‡ä»¶"
                }
            
            table_name = file_path.stem  # ä½¿ç”¨æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰ä½œä¸ºè¡¨å
            schema_info = self.get_dataframe_schema(df_full, table_name)
            
            return {
                "file_name": file_path.name,
                "file_type": "CSV",
                "file_size_mb": round(file_path.stat().st_size / 1024 / 1024, 2),
                "tables": {table_name: schema_info}
            }
            
        except Exception as e:
            return {
                "file_name": file_path.name,
                "file_type": "CSV",
                "tables": {},
                "error": f"æ‰«æCSVæ–‡ä»¶å¤±è´¥: {str(e)}"
            }
    
    def scan_parquet_file(self, file_path: Path) -> Dict[str, Any]:
        """æ‰«æParquetæ–‡ä»¶çš„schema
        
        Args:
            file_path: Parquetæ–‡ä»¶è·¯å¾„
            
        Returns:
            æ–‡ä»¶çš„schemaä¿¡æ¯
        """
        try:
            df = pd.read_parquet(file_path)
            table_name = file_path.stem
            schema_info = self.get_dataframe_schema(df, table_name)
            
            return {
                "file_name": file_path.name,
                "file_type": "PARQUET",
                "file_size_mb": round(file_path.stat().st_size / 1024 / 1024, 2),
                "tables": {table_name: schema_info}
            }
            
        except Exception as e:
            return {
                "file_name": file_path.name,
                "file_type": "PARQUET",
                "tables": {},
                "error": f"æ‰«æParquetæ–‡ä»¶å¤±è´¥: {str(e)}"
            }
    
    def scan_duckdb_file(self, file_path: Path) -> Dict[str, Any]:
        """æ‰«æDuckDBæ–‡ä»¶çš„schema
        
        Args:
            file_path: DuckDBæ–‡ä»¶è·¯å¾„
            
        Returns:
            æ–‡ä»¶çš„schemaä¿¡æ¯
        """
        try:
            conn = duckdb.connect(str(file_path))
            
            # è·å–æ‰€æœ‰è¡¨å
            tables_query = "SELECT table_name FROM information_schema.tables WHERE table_schema = 'main'"
            tables_result = conn.execute(tables_query).fetchall()
            table_names = [row[0] for row in tables_result]
            
            if not table_names:
                conn.close()
                return {
                    "file_name": file_path.name,
                    "file_type": "DUCKDB",
                    "file_size_mb": round(file_path.stat().st_size / 1024 / 1024, 2),
                    "tables": {},
                    "error": "DuckDBæ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°è¡¨"
                }
            
            tables_schema = {}
            for table_name in table_names:
                try:
                    # è·å–è¡¨çš„è¯¦ç»†schemaä¿¡æ¯
                    schema_query = f"DESCRIBE {table_name}"
                    schema_result = conn.execute(schema_query).fetchall()
                    
                    # è·å–è¡¨æ•°æ®ç”¨äºç»Ÿè®¡ä¿¡æ¯
                    df = conn.execute(f"SELECT * FROM {table_name}").df()
                    
                    # æ„å»ºåˆ—ä¿¡æ¯ï¼ˆç»“åˆDuckDBçš„schemaä¿¡æ¯å’Œpandasåˆ†æï¼‰
                    columns_info = {}
                    for row in schema_result:
                        col_name = row[0]
                        col_type = row[1]
                        col_nullable = row[2] == 'YES'
                        
                        # ä»DataFrameè·å–ç»Ÿè®¡ä¿¡æ¯
                        if col_name in df.columns:
                            null_count = int(df[col_name].isnull().sum())
                            unique_count = int(df[col_name].nunique())
                            
                            # å¤„ç†sample_valuesï¼Œç¡®ä¿JSONå¯åºåˆ—åŒ–
                            sample_values = []
                            try:
                                samples = df[col_name].dropna().head(3)
                                for val in samples:
                                    if pd.isna(val):
                                        sample_values.append(None)
                                    elif hasattr(val, 'isoformat'):  # datetimeç±»å‹
                                        sample_values.append(val.isoformat())
                                    elif hasattr(val, 'item'):  # numpyç±»å‹
                                        sample_values.append(val.item())
                                    else:
                                        sample_values.append(str(val))
                            except Exception:
                                sample_values = []
                        else:
                            null_count = 0
                            unique_count = 0
                            sample_values = []
                        
                        columns_info[col_name] = {
                            "data_type": col_type,
                            "nullable": col_nullable,
                            "null_count": null_count,
                            "unique_count": unique_count,
                            "sample_values": sample_values
                        }
                    
                    tables_schema[table_name] = {
                        "table_name": table_name,
                        "row_count": int(df.shape[0]),
                        "column_count": int(df.shape[1]),
                        "columns": columns_info,
                        "memory_usage_mb": round(df.memory_usage(deep=True).sum() / 1024 / 1024, 2)
                    }
                    
                except Exception as e:
                    tables_schema[table_name] = {
                        "table_name": table_name,
                        "error": f"æ‰«æè¡¨å¤±è´¥: {str(e)}"
                    }
            
            conn.close()
            
            return {
                "file_name": file_path.name,
                "file_type": "DUCKDB",
                "file_size_mb": round(file_path.stat().st_size / 1024 / 1024, 2),
                "tables": tables_schema
            }
            
        except Exception as e:
            return {
                "file_name": file_path.name,
                "file_type": "DUCKDB",
                "tables": {},
                "error": f"è¿æ¥DuckDBæ–‡ä»¶å¤±è´¥: {str(e)}"
            }
    
    def scan_all_databases(self) -> Dict[str, Any]:
        """æ‰«ææ‰€æœ‰æ•°æ®åº“æ–‡ä»¶çš„schema
        
        Returns:
            å®Œæ•´çš„æ•°æ®åº“schemaå­—å…¸
        """
        print(f"ğŸ” å¼€å§‹æ‰«ææ•°æ®ç›®å½•: {self.data_dir}")
        
        data_files = self.get_data_files()
        if not data_files:
            return {
                "scan_summary": {
                    "total_files": 0,
                    "total_tables": 0,
                    "scan_status": "æ²¡æœ‰æ‰¾åˆ°æ”¯æŒçš„æ•°æ®æ–‡ä»¶"
                },
                "databases": {}
            }
        
        print(f"ğŸ“ æ‰¾åˆ° {len(data_files)} ä¸ªæ•°æ®æ–‡ä»¶")
        
        databases_schema = {}
        total_tables = 0
        
        for file_path in data_files:
            print(f"\nğŸ”„ æ‰«ææ–‡ä»¶: {file_path.name}")
            
            if file_path.suffix.lower() == '.csv':
                schema_info = self.scan_csv_file(file_path)
            elif file_path.suffix.lower() == '.parquet':
                schema_info = self.scan_parquet_file(file_path)
            elif file_path.suffix.lower() in ['.duckdb', '.db']:
                schema_info = self.scan_duckdb_file(file_path)
            else:
                continue
            
            databases_schema[file_path.name] = schema_info
            
            # ç»Ÿè®¡è¡¨æ•°é‡
            if 'tables' in schema_info and isinstance(schema_info['tables'], dict):
                total_tables += len(schema_info['tables'])
                print(f"âœ“ æ‰«æå®Œæˆ: {len(schema_info['tables'])} ä¸ªè¡¨")
            else:
                print(f"âš  æ‰«æå¤±è´¥æˆ–æ— è¡¨")
        
        # æ„å»ºå®Œæ•´çš„schemaç»“æœ
        result = {
            "scan_summary": {
                "total_files": len(data_files),
                "total_tables": total_tables,
                "scan_status": "æ‰«æå®Œæˆ",
                "data_directory": str(self.data_dir)
            },
            "databases": databases_schema
        }
        
        print(f"\nğŸ‰ Schemaæ‰«æå®Œæˆï¼å…±æ‰«æ {len(data_files)} ä¸ªæ–‡ä»¶ï¼Œ{total_tables} ä¸ªè¡¨")
        return result
    
    def print_schema_summary(self, schema_dict: Dict[str, Any]):
        """æ‰“å°schemaæ‘˜è¦ä¿¡æ¯
        
        Args:
            schema_dict: scan_all_databasesè¿”å›çš„schemaå­—å…¸
        """
        print("\n" + "="*80)
        print("ğŸ“Š æ•°æ®åº“Schemaæ‰«æç»“æœæ‘˜è¦")
        print("="*80)
        
        summary = schema_dict.get('scan_summary', {})
        print(f"ğŸ“ æ‰«æç›®å½•: {summary.get('data_directory', 'Unknown')}")
        print(f"ğŸ“„ æ–‡ä»¶æ€»æ•°: {summary.get('total_files', 0)}")
        print(f"ğŸ—ƒï¸  è¡¨æ€»æ•°: {summary.get('total_tables', 0)}")
        print(f"ğŸ“Š æ‰«æçŠ¶æ€: {summary.get('scan_status', 'Unknown')}")
        
        databases = schema_dict.get('databases', {})
        
        for file_name, db_info in databases.items():
            print(f"\nğŸ“‹ æ–‡ä»¶: {file_name} ({db_info.get('file_type', 'Unknown')})")
            print(f"   å¤§å°: {db_info.get('file_size_mb', 0)} MB")
            
            if 'error' in db_info:
                print(f"   âŒ é”™è¯¯: {db_info['error']}")
                continue
            
            tables = db_info.get('tables', {})
            print(f"   è¡¨æ•°é‡: {len(tables)}")
            
            for table_name, table_info in tables.items():
                if 'error' in table_info:
                    print(f"     âŒ è¡¨ {table_name}: {table_info['error']}")
                    continue
                
                print(f"     ğŸ“Š è¡¨ {table_name}:")
                print(f"        è¡Œæ•°: {table_info.get('row_count', 0):,}")
                print(f"        åˆ—æ•°: {table_info.get('column_count', 0)}")
                print(f"        å†…å­˜: {table_info.get('memory_usage_mb', 0)} MB")
                
                # æ˜¾ç¤ºå‰å‡ ä¸ªåˆ—çš„ä¿¡æ¯
                columns = table_info.get('columns', {})
                if columns:
                    print(f"        ä¸»è¦å­—æ®µ:")
                    for i, (col_name, col_info) in enumerate(list(columns.items())[:5]):
                        data_type = col_info.get('data_type', 'Unknown')
                        nullable = "å¯ç©º" if col_info.get('nullable', False) else "éç©º"
                        print(f"          {col_name}: {data_type} ({nullable})")
                    
                    if len(columns) > 5:
                        print(f"          ... è¿˜æœ‰ {len(columns) - 5} ä¸ªå­—æ®µ")
    
    def export_schema_to_json(self, schema_dict: Dict[str, Any], output_file: str = "database_schema.json"):
        """å¯¼å‡ºschemaåˆ°JSONæ–‡ä»¶
        
        Args:
            schema_dict: schemaå­—å…¸
            output_file: è¾“å‡ºæ–‡ä»¶å
        """
        import json
        
        try:
            # ç¡®ä¿è¾“å‡ºåˆ°å½“å‰è„šæœ¬æ‰€åœ¨çš„prepareç›®å½•
            output_path = Path(__file__).parent / output_file
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(schema_dict, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ’¾ Schemaå·²å¯¼å‡ºåˆ°: {output_path}")
        except Exception as e:
            print(f"\nâŒ å¯¼å‡ºå¤±è´¥: {str(e)}")


def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨Schemaæ‰«æå™¨"""
    try:
        # åˆ›å»ºæ‰«æå™¨å®ä¾‹
        scanner = DatabaseSchemaScanner()
        
        # æ‰«ææ‰€æœ‰æ•°æ®åº“
        schema_result = scanner.scan_all_databases()
        
        # æ‰“å°æ‘˜è¦ä¿¡æ¯
        scanner.print_schema_summary(schema_result)
        
        # å¯¼å‡ºåˆ°JSONæ–‡ä»¶
        scanner.export_schema_to_json(schema_result)
        
        return schema_result
        
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        return None


if __name__ == "__main__":
    main()